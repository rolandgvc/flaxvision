{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "!pip install --upgrade -q pip jax jaxlib\n",
    "!pip install --upgrade -q git+https://github.com/google/flax.git    \n",
    "!pip install --upgrade -q git+https://github.com/rolandgvc/flaxvision.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from flax import linen as nn\n",
    "from flax import optim\n",
    "from flaxvision import models\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from torchvision into memory\n",
    "train_ds = datasets.MNIST('./data', train=True, download=True)\n",
    "test_ds = datasets.MNIST('./data', train=False, download=True)\n",
    "train_ds = {'image': np.expand_dims(train_ds.data.numpy(), 3), \n",
    "            'label': train_ds.targets.numpy()}\n",
    "test_ds = {'image': np.expand_dims(test_ds.data.numpy(), 3), \n",
    "           'label': test_ds.targets.numpy()}\n",
    "train_ds['image'] = jnp.float32(train_ds['image']) / 255.\n",
    "test_ds['image'] = jnp.float32(test_ds['image']) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pretrained model\n",
    "RNG = jax.random.PRNGKey(0)\n",
    "vgg, vgg_params = models.vgg16(RNG, pretrained=True)\n",
    "\n",
    "#TODO: test with an image from dataset\n",
    "batch = jnp.ones((1, 224, 224, 3))\n",
    "out = vgg.apply(vgg_params, batch, mutable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define backbone instantization as a lambda function\n",
    "vgg_backbone = lambda: models.VGG.make_backbone(vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new model\n",
    "from flax import linen as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  dtype: Any = jnp.float32\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, inputs, train: bool = False):\n",
    "    x = nn.Dense(2048, dtype=self.dtype)(inputs)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(rate=0.5)(x, deterministic=not train)\n",
    "    x = nn.Dense(2048, dtype=self.dtype)(x)\n",
    "    x = nn.relu(x)\n",
    "    x = nn.Dropout(rate=0.5)(x, deterministic=not train)\n",
    "    x = nn.Dense(10, dtype=self.dtype)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "  def setup(self):\n",
    "    self.backbone = vgg_backbone()\n",
    "    self.classifier = Classifer()\n",
    "\n",
    "  def __call__(self, inputs, train: bool = False)\n",
    "    x = self.backbone(inputs, train=False)\n",
    "    x = x.transpose((0, 3, 1, 2))\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = self.classifier(x, train)\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training loop\n",
    "def get_initial_params(key):\n",
    "  init_shape = jnp.ones((1, 224, 224, 3), jnp.float32)\n",
    "  initial_params = MyModel().init(key, init_shape)['params']\n",
    "  return initial_params\n",
    "\n",
    "\n",
    "def create_optimizer(params, learning_rate, beta):\n",
    "  optimizer_def = optim.Momentum(learning_rate=learning_rate, beta=beta)\n",
    "  optimizer = optimizer_def.create(params)\n",
    "  return optimizer\n",
    "\n",
    "\n",
    "def onehot(labels, num_classes=10):\n",
    "  x = (labels[..., None] == jnp.arange(num_classes)[None])\n",
    "  return x.astype(jnp.float32)\n",
    "\n",
    "\n",
    "def cross_entropy_loss(logits, labels):\n",
    "  return -jnp.mean(jnp.sum(onehot(labels) * logits, axis=-1))\n",
    "\n",
    "\n",
    "def compute_metrics(logits, labels):\n",
    "  loss = cross_entropy_loss(logits, labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def train_step(optimizer, batch):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  def loss_fn(params):\n",
    "    logits = CNN().apply({'params': params}, batch['image'])\n",
    "    loss = cross_entropy_loss(logits, batch['label'])\n",
    "    return loss, logits\n",
    "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "  (_, logits), grad = grad_fn(optimizer.target)\n",
    "  optimizer = optimizer.apply_gradient(grad)\n",
    "  metrics = compute_metrics(logits, batch['label'])\n",
    "  return optimizer, metrics\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "  logits = CNN().apply({'params': params}, batch['image'])\n",
    "  return compute_metrics(logits, batch['label'])\n",
    "\n",
    "\n",
    "def train_epoch(optimizer, train_ds, batch_size, epoch, rng):\n",
    "  \"\"\"Train for a single epoch.\"\"\"\n",
    "  train_ds_size = len(train_ds['image'])\n",
    "  steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
    "  perms = perms[:steps_per_epoch * batch_size]  # skip incomplete batch\n",
    "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "  batch_metrics = []\n",
    "  for perm in perms:\n",
    "    batch = {k: v[perm] for k, v in train_ds.items()}\n",
    "    optimizer, metrics = train_step(optimizer, batch)\n",
    "    batch_metrics.append(metrics)\n",
    "\n",
    "  # compute mean of metrics across each batch in epoch.\n",
    "  batch_metrics_np = jax.device_get(batch_metrics)\n",
    "  epoch_metrics_np = {\n",
    "      k: onp.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]}\n",
    "\n",
    "  logging.info('train epoch: %d, loss: %.4f, accuracy: %.2f', epoch,\n",
    "               epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100)\n",
    "\n",
    "  return optimizer, epoch_metrics_np\n",
    "\n",
    "\n",
    "def eval_model(model, test_ds):\n",
    "  metrics = eval_step(model, test_ds)\n",
    "  metrics = jax.device_get(metrics)\n",
    "  summary = jax.tree_map(lambda x: x.item(), metrics)\n",
    "  return summary['loss'], summary['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run training loop\n",
    "\n",
    "# summary_writer = tensorboard.SummaryWriter()\n",
    "\n",
    "rng, init_rng = jax.random.split(PRNG)\n",
    "params = get_initial_params(init_rng)\n",
    "optimizer = create_optimizer(params, learning_rate, momentum)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "    optimizer, train_metrics = train_epoch(optimizer, train_ds, batch_size, \n",
    "                                           epoch, input_rng)\n",
    "    loss, accuracy = eval_model(optimizer.target, test_ds)\n",
    "\n",
    "    summary_writer.scalar('train_loss', train_metrics['loss'], epoch)\n",
    "    summary_writer.scalar('train_accuracy', train_metrics['accuracy'], epoch)\n",
    "    summary_writer.scalar('eval_loss', loss, epoch)\n",
    "    summary_writer.scalar('eval_accuracy', accuracy, epoch)\n",
    "\n",
    "    summary_writer.flush()\n",
    "    \n",
    "    if (step + 1) % steps_per_checkpoint == 0 or step + 1 == num_steps:\n",
    "      state = sync_batch_stats(state)\n",
    "      step = int(state.step)\n",
    "      checkpoints.save_checkpoint(workdir, state, step, keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from checkpoint and inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
